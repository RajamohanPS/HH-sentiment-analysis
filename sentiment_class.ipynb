{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zQpIt8RO85o"
   },
   "source": [
    "# Sentiment classification of YT comments/ tweets/ reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3b9ZEqOaO85o"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZL8ZoxZOaC0F"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0EXbp1WqO85p"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0B9Od6zgLtzK"
   },
   "source": [
    "## Loading raw data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UlbOA3GgO85q"
   },
   "outputs": [],
   "source": [
    "# did these separately, so wrote the code as such\n",
    "# comment those not needed\n",
    "\n",
    "# for yt-data\n",
    "yt_data = pd.read_csv('raw_data/yt-comment-data2.csv')\n",
    "yt_data.drop('id',axis=1,inplace=True)\n",
    "yt_data.rename(columns={'comments': 'raw_content'}, inplace=True)\n",
    "yt_data['source'] = 'yt'\n",
    "\n",
    "# for twitter data\n",
    "tw_data = pd.read_csv('raw_data/twitter.csv')\n",
    "tw_data.drop(columns=['timestamp','query'],axis=1,inplace=True)\n",
    "tw_data = tw_data[(tw_data['twitterProfile'] != 'https://twitter.com/henry_education')\n",
    " & (tw_data['twitterProfile'] != 'https://twitter.com/henryharvin_in')]\n",
    "tw_data.rename(columns={'content': 'raw_content'}, inplace=True)\n",
    "tw_data['source'] = 'tw'\n",
    "\n",
    "# for trust pilot data\n",
    "tp_data = pd.read_csv('raw_data/trust-pilot.csv')\n",
    "tp_data['content'] = tp_data['Content1'].fillna(tp_data['Content'])\n",
    "tp_data['info'] = tp_data['Info1'].fillna(tp_data['Info'])\n",
    "tp_data = tp_data[['info', 'content']]\n",
    "tp_data.rename(columns={'content': 'raw_content'}, inplace=True)\n",
    "tp_data['source'] = 'tp'\n",
    "\n",
    "#data = yt_data.copy() # replace for separate classification\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "p1MW7V4RXx_m",
    "outputId": "4594762a-2d51-4e4f-a684-bb4af82468e0"
   },
   "outputs": [],
   "source": [
    "# or concat to classify together\n",
    "data = pd.concat([yt_data[['raw_content', 'source']],\n",
    "                  tw_data[['raw_content', 'source']],\n",
    "                  tp_data[['raw_content', 'source']]])\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0vzP5pmO85p"
   },
   "source": [
    "## Cleaning data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3qDmNuOIO85q"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    #lower-case\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove handles, url's\n",
    "    text= re.sub(r'@\\S+', '',text)\n",
    "    text= re.sub(r'http\\S+', '',text) \n",
    "    text= re.sub(r'pic.\\S+', '',text)\n",
    "    \n",
    "    # removing #tags \n",
    "    #text= re.sub(r'#\\S+', '',text)\n",
    "    \n",
    "    # replace unidecode characters\n",
    "    text=unidecode.unidecode(text)\n",
    "      \n",
    "    # regex only keeps characters\n",
    "    text= re.sub(r\"[^a-zA-Z+']\", ' ',text)\n",
    "    \n",
    "    #removing 'br' 'href' html\n",
    "    text= re.sub(r'br', '',text)\n",
    "    text= re.sub(r'href', '',text)\n",
    "    \n",
    "    # keep words with length>1 only\n",
    "    text=re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text+' ')\n",
    "    \n",
    "    # regex removes repeated spaces, strip removes leading and trailing spaces\n",
    "    text= re.sub(r\"\\s[\\s]+\", \" \",text).strip()\n",
    "    \n",
    "    # keep words with length>1 only\n",
    "    text=re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text+' ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8EgY5648O85r",
    "outputId": "233ed868-a5cb-48bc-89d9-f96128b6091f"
   },
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)\n",
    "data['content'] = data['raw_content'].apply(lambda x: clean_text(x))\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6hQhtcwAO85r",
    "outputId": "f06b7098-1a3e-4bc2-f9f3-582b49d2f0c9"
   },
   "outputs": [],
   "source": [
    "# final cleaning on content\n",
    "\n",
    "# stripping spaces\n",
    "data['content'] = data['content'].apply(lambda x: x.strip())\n",
    "# only taking text with greater than 3 letters after cleaning\n",
    "data = data[data['content'].apply(lambda x: len(x) > 3)]\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuwQZKDxO85s"
   },
   "source": [
    "## Sentiment Classification: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoQB8sguO85s"
   },
   "source": [
    "### Polarity, Subjectivity & Sentiment using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VfTmpYgGO85s"
   },
   "outputs": [],
   "source": [
    "# function to get sentiment scores \n",
    "def sentiment_scores(text, pol=None, subj=None):\n",
    "    \n",
    "    if pol is None:\n",
    "        pol = []\n",
    "    if subj is None:\n",
    "        sunj = []\n",
    "        \n",
    "    #using textblob for scores    \n",
    "    sent = TextBlob(text)\n",
    "    pol.append(sent.sentiment.polarity)\n",
    "    subj.append(sent.sentiment.subjectivity)\n",
    "    \n",
    "    return pol, subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SoDVT5_OO85s"
   },
   "outputs": [],
   "source": [
    "# function to return polarity, subjectivity\n",
    "def list_scores(lst):\n",
    "    \n",
    "    pol = []\n",
    "    subj = []\n",
    "    \n",
    "    for i in lst:\n",
    "        pol, subj = sentiment_scores(i, pol, subj)\n",
    "        \n",
    "    return pol, subj\n",
    "\n",
    "polarity, subjectivity = list_scores(data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlwUwczZO85s",
    "outputId": "b2536abd-236a-48c8-a40b-bdf0fb03d1c0"
   },
   "outputs": [],
   "source": [
    "# dataframe with results of textblob analysis\n",
    "sentiment_data = data[['source', 'content']].copy()\n",
    "sentiment_data['tb_polarity'] = polarity\n",
    "sentiment_data['tb_subjectivity'] = subjectivity\n",
    "#sentiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZP-zup7O85t"
   },
   "source": [
    "Using 'pol < 0' for negative results in a lot of false negative sentiments <br>\n",
    "Use a custom threshold if possible <br>\n",
    "Edit: later comparing with other sentiment models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WB_WjASYO85t"
   },
   "outputs": [],
   "source": [
    "# assign sentiment based on polarity\n",
    "def decode_sentiment(pol,neg_th=0,pos_th=0):\n",
    "    if pol < neg_th:\n",
    "        return 'Negative'\n",
    "    elif pol > pos_th:\n",
    "        return 'Positive'\n",
    "    return 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXtrtodnO85t",
    "outputId": "38ce428c-128b-4606-ed98-dd832994f0fb"
   },
   "outputs": [],
   "source": [
    "sentiment_data['tb_sentiment'] = sentiment_data['tb_polarity'].apply(lambda x: decode_sentiment(x))\n",
    "#sentiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJa65Ba9O85t",
    "outputId": "d2f68af9-3d07-406e-cf47-9e50986bf0a2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking negative sentiments\n",
    "#sentiment_data[sentiment_data['tb_sentiment']=='Negative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEKHe8ndO85t"
   },
   "source": [
    "### Sentiment classification using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qDjMYAEdO85t"
   },
   "outputs": [],
   "source": [
    "#nltk.download('opinion_lexicon') if not done already\n",
    "\n",
    "# return sentiment of comment based on sentiment of each token\n",
    "def nltk_sentiment(sentence):\n",
    "    tokenizer = treebank.TreebankWordTokenizer()\n",
    "    pos_words = 0\n",
    "    neg_words = 0\n",
    "    tokenized_sent = [word.lower() for word in tokenizer.tokenize(sentence)]\n",
    "\n",
    "    x = list(range(len(tokenized_sent)))\n",
    "    y = []\n",
    "\n",
    "    # getting sentiment of each token using \n",
    "    for word in tokenized_sent:\n",
    "        if word in opinion_lexicon.positive():\n",
    "            pos_words += 1\n",
    "            y.append(1)  # positive\n",
    "        elif word in opinion_lexicon.negative():\n",
    "            neg_words += 1\n",
    "            y.append(-1)  # negative\n",
    "        else:\n",
    "            y.append(0)  # neutral\n",
    "\n",
    "    # returns sentiment based on count of pos_words/neg_words\n",
    "    if pos_words > neg_words:\n",
    "        return \"Positive\"\n",
    "    elif pos_words < neg_words:\n",
    "        return \"Negative\"\n",
    "    elif pos_words == neg_words:\n",
    "        return \"Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "843IBmYoO85u"
   },
   "outputs": [],
   "source": [
    "# return list of sentiments of all content\n",
    "def list_nltk(lst):\n",
    "    \n",
    "    pol = []\n",
    "    \n",
    "    for i in lst:\n",
    "        pol.append(nltk_sentiment(i))\n",
    "        \n",
    "    return pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sW6JitGOO85u"
   },
   "outputs": [],
   "source": [
    "nltk_pol = list_nltk(data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "E3OSDbQbO85u"
   },
   "outputs": [],
   "source": [
    "# dataframe with all nltk analysis\n",
    "nltk_data = data.copy()\n",
    "nltk_data['sentiment'] = nltk_pol\n",
    "#nltk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "tf3C7FBZO85u"
   },
   "outputs": [],
   "source": [
    "# checking negative sentiments\n",
    "#nltk_data[nltk_data['sentiment']=='Negative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0wZEqXfSzNO"
   },
   "source": [
    "### Comparing sentiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "eMcI3KeIO85u"
   },
   "outputs": [],
   "source": [
    "# comparing results of textblob and nltk\n",
    "compare_data = data.copy()\n",
    "compare_data['tb_sentiment'] = sentiment_data['tb_sentiment']\n",
    "compare_data['nltk_sentiment'] = nltk_data['sentiment']\n",
    "\n",
    "#compare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "pNHfYf9vO85v"
   },
   "outputs": [],
   "source": [
    "compare_data['sentiment'] = compare_data['tb_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ytzDxNX7O85v"
   },
   "outputs": [],
   "source": [
    "# using comparison of textblob and nltk to choose best sentiment option\n",
    "def tb_nltk_sentiment(df):\n",
    "    \n",
    "    # lists to iterate\n",
    "    tb_sent = list(df['tb_sentiment'])\n",
    "    nltk_sent = list(df['nltk_sentiment'])\n",
    "    comments = list(df['content'])\n",
    "    sentiments = []\n",
    "    \n",
    "    # these modifications are to classify questions as neutral\n",
    "    questions = ['what', 'who', 'why', 'when', 'is', 'how', 'can']\n",
    "    thanks = ['thank', 'thanks', 'thankyou', 'thanku']\n",
    "    \n",
    "    # sentiments\n",
    "    for i in range(len(comments)):\n",
    "        # giving questions 'Neutral' sentiment\n",
    "        if comments[i].split(' ')[0] in questions:\n",
    "            sentiments.append('Neutral')\n",
    "        # giving comments of thanks 'Positive' sentiment\n",
    "        elif comments[i].split(' ')[0] in thanks:\n",
    "            sentiments.append('Positive')\n",
    "\n",
    "        # sentiments based on comparison, fit to reduce errors\n",
    "        elif tb_sent[i] == nltk_sent[i]:\n",
    "            sentiments.append(nltk_sent[i])\n",
    "        elif (tb_sent[i] == 'Negative'):\n",
    "            if (nltk_sent[i] == 'Neutral'):\n",
    "                sentiments.append('Negative')\n",
    "            else:\n",
    "                sentiments.append('Neutral')\n",
    "        \n",
    "        elif (nltk_sent[i] == 'Neutral'):\n",
    "            sentiments.append('Positive') \n",
    "        else:\n",
    "            sentiments.append(nltk_sent[i])\n",
    "    \n",
    "    # adding column to dataframe\n",
    "    df['sentiment'] = sentiments\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "eg2OBDNaO85w"
   },
   "outputs": [],
   "source": [
    "compare_data = tb_nltk_sentiment(compare_data)\n",
    "#compare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "uLYY-gHLO85w"
   },
   "outputs": [],
   "source": [
    "# checking negative sentiment comments\n",
    "#(compare_data[compare_data['sentiment'] == 'Negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "-27Q6XRNO85w"
   },
   "outputs": [],
   "source": [
    "# combining results to sentiment_data dataframe\n",
    "sentiment_data['nltk_sentiment'] = compare_data['nltk_sentiment']\n",
    "sentiment_data['final_sentiment'] = compare_data['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ii_Zkct4TnEV"
   },
   "source": [
    "## Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "956wvk6NO85x"
   },
   "outputs": [],
   "source": [
    "# write to csv file\n",
    "sentiment_data.to_csv('output/sentiment.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0zQpIt8RO85o",
    "0B9Od6zgLtzK",
    "f0vzP5pmO85p",
    "YuwQZKDxO85s",
    "hoQB8sguO85s",
    "bEKHe8ndO85t",
    "Q0wZEqXfSzNO",
    "Ii_Zkct4TnEV"
   ],
   "name": "sentiment_class.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
